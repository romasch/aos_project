\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}


% For the \todo{} command.
\usepackage{todo}

% Nice fonts
\usepackage{palatino}
% Needed for Listings package with Eiffel.
% \usepackage{xcolor}
% Source code listings.
\usepackage{listings}
% Appendix with extra title.
% \usepackage [page] {appendix}
% To include PNG files.
% \usepackage{graphicx}
% Nice looking captions.
\usepackage[font={footnotesize,sl}, labelfont=bf] {caption}
% Include PDF pages.
% \usepackage{pdfpages}

% Clickable links. Has to be the last package:
\usepackage [hidelinks] {hyperref}


\lstset{language=C,basicstyle=\ttfamily\small}


\newcommand{\todoref}{\todo{ref}}
\newcommand{\filepath}[1]{\emph{ #1}}

% Title Page
\title{Advanced Operating Systems \\ Project Report}
\author{Roman Schmocker \\ Yauhen Klimiankou}


\begin{document}

\maketitle


\section{Introduction}

This report provides documentation for the operating system which we built as part of the project in the Advanced Operating Systems course.
The code is based on a stripped-down version of Barrelfish \cite {web:barrelfish}.

\todo{ more introduction}

\section{Getting started}

\begin{lstlisting}

void test_listings_package (void);

void test_listings_package (void)
{
  uint32_t an_int;
  char* a_string = "asdf";
  for (int i=0; i<42; i++) {
    printf (a_string);
  }
}
\end{lstlisting}


\section {Modules}

\subsection{Paging}

% 
% - Intent: quick lookup of page table, ``unmap'' requests from memory server
% 
% - Data structues
% -- Simple for virtual memory
% -- 2-level array for page tables (mirroring actual page tables)
% -- linked list for frames
% 
% - Problem: No unmap from domain itself
% -- esp. no recycling of virtual addresses
% -- we thought that malloc would never return memory anyway
% -- didn't think about domain spawning / bulk transfer with explicit map/unmap operations

The paging code is mostly contained in \filepath{lib/barrelfish/paging.c} and the corresponding header file.
The central element is the \lstinline!paging_state! struct.
When desgining the data structure for the paging code we wanted to support the following functions in constant time:

\begin{itemize}
 \item Check if an address is valid.
 \item Lookup of the page tables associated to the virtual address.
 \item Allocation of a physical frame for a page.
 \item Deallocation of a frame when the memory server asks for it.
\end{itemize}

To support the first use case we just keep track of the allocated range of virtual addresses using two integer variables.
That way determining if a page fault is ``valid'' is very simple: 
If it's in the range, everything is ok and we can map the page to a frame, and if not we abort due to an invalid address.

The data structure of choice to represent the page table structure is a two-level array that mirrors the actual page tables in memory.
Currently we only store the capability to the page tables and whether a certain page has been allocated at some point.
We don't yet store any flags, pointers to a swap file or other metadata, but it would be easy to add this information.

Frame management is a bit more complex because we always allocate frames of 1 MiB.
That way we can avoid an IPC call for every single page fault, but we also have to keep track of free and allocated space within a frame.
The \lstinline!struct frame_list! manages this information for a single frame.
To support the last use case it also keeps track of the actual pages stored on this frame.
The \lstinline!paging_state! struct then maintains a double ended queue of \lstinline!frame_list! nodes, where only the frame at the head of the list may contain some free space.

As already mentioned, an important use case we had in mind were requests to return a frame from the memory server.
We never implemented that functionality, but the data structure is designed to support it.
An implementation would have to do the following:
\begin{itemize}
 \item Remove the last \lstinline!frame_list! element.
 \item Write all pages backed by this frame to disk.
 \item (Optionally) update some flags and metadata in the page table array.
 \item Return the frame to the memory server.
\end{itemize}

The data structures used to track frames and page tables need some form of dynamic memory management.
Unfortunately however we can't use malloc/free in our paging implementation, because this memory is not initially mapped.
Therefore we used slab allocators (from \filepath{include/barrelfish/slab.h}) to manage the memory needed by these data structures.
The feature \lstinline!memory_refill! in our paging code is used to enlarge the pool of usable memory if the slab allocators run out of space.

\subsubsection{Assessment}

Our initial design of the paging module turned out to be rather flexible.
Over the course of the project, we only had to change small things, like adding another slab allocator for exception stacks of user-level threads 
or implementing \lstinline!paging_map_fixed_attr!, where we could adapt an existing function that did almost the same job.

The only thing that proved to be insufficient was the management of virtual address space.
The initial assumption that only malloc/free can issue unmap operations (and never does as it's not implemented) was wrong.
There are some parts in the code that directly interacts with the paging library and needs to ``free'' a range of virtual memory sometimes, among them the spawndomain library or the bulk transfer mechanism.

Supporting this would have required us to rewrite a lot of existing code, and that's why we didn't do it in the end.
This in turn means that our current solution leaks virtual addresses, but we tried to reduce this as much as possible by reusing the ranges of virtual memory.

\subsection{Local Message Passing}

The local message passing (LMP) system is mostly implemented in \filepath{lib/barrelfish/aos\_rpc.c} for the client side.
Some generic server-side features are implemented in \filepath{lib/aos\_support/server.c}.

The LMP system has some important characteristics:
\begin{itemize}
 \item All messages are synchronous, i.e. receive is blocking.
 \item The channels are always one-to-one connections.
 \item The communication is client-server style.
 \item We support direct connections to any server (i.e. no indirection through init).
 \item Every ``send'' message has an enum constant as its first argument that denotes the message type.
 \item Every ``reply'' message has an error code as its first argument.
 \item All other arguments are determined by the actual message type.
\end{itemize}

The individual message types and their arguments are described in the header file \filepath{include/barrelfish/aos\_rpc.h}.
To support writing code on the client side we implemented the feature \lstinline!aos_send_receive! which takes a struct containint all arguments, 
sends a request to the server, waits for a reply, and copies the receive arguments back into the struct.
That way we could avoid having to write the same functionality for every message type.

Some servers provide core system services. Init for exampleprovides RAM, and the serial\_driver provides I/O.
We therefore provide some predefined channels to these services which are initialized in \filepath{lib/barrelfish/init.c}.
The channels can be retrieved with the functions \lstinline!aos_rpc_get_init_channel! and \lstinline!aos_rpc_get_serial_driver_channel!, respectively.

\subsubsection{Connection setup and name service}

In our LMP implementation we wanted to support direct channels between two domains.
To do that we had to implement some additional functionality, in particular a simple name service and a mechanism to set up a new connection.

The design of the name service is very simple.
In \filepath{aos\_rpc.h} we added an enum type \lstinline!aos_service! which is used for some of the core system services, such as the serial driver or the FAT file system.
Each domain that acts as a server for such a core system service then has to register itself via the \lstinline!AOS_RPC_REGISTER_SERVICE! LMP message.
Init itself keeps track of all registered servers and therefore acts as a name service.

When a client wants to find a particular service, it sends a request to init, and init replies with a new endpoint capability of the specified domain.
This was a bit tricky to implement, because we only allow one-to-one connections, so init first has to request a new endpoint from the (registered) server.
Init shouldn't mix the reply however with some other request from another domain, therefore we had to break our usual send-reply protocol and plan this interaction carefully.

In chronological order, this is what happens when a domain wants to find, for example, the serial driver:

\begin{itemize}
 \item The client sends an \lstinline!AOS_RPC_FIND_SERVICE! with the enum constant of the serial driver to init.
 \item Init uses a new request ID (a ``cookie'') and remembers the channel that sent the request.
 \item Init checks if the serial driver is registered and sends an \lstinline!AOS_ROUTE_REQUEST_EP!.
 This call includes the previously generated request ID.
 \item The serial driver handles the enpoint request call by creating a new channel.
 It replies to init with an \lstinline!AOS_ROUTE_DELIVER_EP! which contains the new endpoint and the unchanged request ID as arguments.
 Note that this RPC call doesn't follow the usual send-reply style message exchange, because the reply contains a message type argument.
 \item Thanks to the special reply format init can handle the request like any other LMP message.
 When receiving the \lstinline!AOS_ROUTE_DELIVER_EP!, init retrieves the channel that initially sent the \lstinline!AOS_RPC_FIND_SERVICE! request using the request ID and forwards the endpoint.
 \item The client domain wakes up again receives the new endpoint, which it can then use to establish a direct connection to the serial driver.
\end{itemize}


% - Synchronous message passing, one-to-one
% - channels between any client/server possible (not just init)
% - general format: service identifier, args -> reply: error code, args
% - aos\_rpc.c: client API
% -- support feature \lstinline!aos_send_receive!
% -- only initialize arguments and receive result.
% - aos\_support/server.c: generic server support routines
% - routing: in init - service registration, find requests
% - bulk transfer: shared buffer, bound to a channel
% - predefined channels (provided by libbarrelfish): init, serial\_driver


\subsubsection{Bulk transfer}

For a long time throughout the project we only had the basic message-passing communication model.
Although this worked well, we always had to split an IPC call into several messages if the data to be transmitted exceeded the capacity of a single message.
This was a tedious and error prone process, and due to this we decided to add a mechanism for bulk transfer.

The basic idea is to set up a buffer which is shared between client and server.
This setup is initiated by the client, and only when it is actually needed.

We added a new message type which allowed a client to share a frame with a buffer and get back a \lstinline!memory_descriptor!, which it can use afterwards for other IPC calls.
The server-side part is implemented in \filepath{lib/aos\_support/}, which had the nice side effect that every server gained the buffer sharing mechanism for free without any further changes.
The client-side part is implemented in \filepath{lib/barrelfish/aos\_rpc.c}, where it is now possible to ``attach'' a shared buffer to any \lstinline!struct aos_rpc! channel.

The bulk transfer mechanism was a pretty late addition to the code base, therefore not all RPC calls have been changed to make use of it yet.


\subsection{Shell and serial I/O}

The shell is implemented in \filepath{usr/memeater/memeater.c}.
It is started after init has initialized the core system services.

The shell application basically executes an endless loop where it requests a character from the serial driver, stores it in a buffer, and echoes back the character.
When the shell recognizes a newline character however it starts to evaluate the command.
First it checks if the input matches one of its builtin commands, using a big \lstinline!if-else! construct.
If no match is found the shell proceeds by trying to execute a binary with the name entered by the user (see Section \ref{sec:process-management}).

The serial driver is responsible for terminal input and output functinality.
It is located in \filepath{usr/serial\_driver/}.
The implementation of the serial driver is quite simple, as we didn't implement interrupts.
Therefore the server just polls the serial device when a domain requests a character, or writes output characters into a dedicated device register.

Besides simple character transfer the serial driver also serves as the target of \lstinline!aos_rpc_set_foreground! to determine a single domain that can receive input and \lstinline!aos_rpc_send_string! to print a string.
The latter makes use of the bulk transfer mechanism to avoid splitting and transmitting the string in small chunks.

\todo{Mention that printf and scanf in libc is re-routed to use serial driver.}
% - Implemented in memeater.c
% - serial driver in own domain
% - \lstinline!set_foreground! for input selection
% -- disadvantage: not very safe...
% - big if-elseif construct to parse commands...
% - \lstinline!aos_rpc_send_string! to send a large string to serial driver, but currently not used

\subsection{Process management}
\label{sec:process-management}
- currently done in init
- server manages process table and info about loaded modules
-- can be printed with ps command
- shell attempts to start program with specific name if command not recognized
- mechanism to register for end notification (foreground tasks)
- kill command to revoke dispatcher capability (background tasks)
- no further cleanup for zombie domains as for now

\subsection{Mupltiple Cores}

\subsubsection{Preparing the kernel image}

The preparation of the kernel image is mostly done in \filepath{usr/init/cross\_core\_setup.c}.
The implementation is heavily inspired\footnote{copy-pasted} from the upstream Barrelfish code base.
Compared to the upstream mechanism we implemented some simplifications however, since we don't have to support more than two cores.
We also adapted the code a bit to our own data structures.

\subsubsection{Low-level mechanisms}

To start the second core one has to call the \lstinline!sys_boot_core! system call, which eventually executes \lstinline!start_aps_arm_start! in the first kernel.
Within this function there's no magic going on - it just writes the start address for the second core to a regsiter and issues an interrupt.

The real magic happens in \lstinline!arch_init! in \filepath{kernel/arch/omap44xx/init.c} and  \lstinline{arm_kernel_startup} in \filepath{kernel/arch/omap44xx/startup\_arch.c}.
In \lstinline!arch_init! we set up the \lstinline!global! struct which contains data to be shared between both kernels.
Among the data shared between the two kernels is the multiboot image.
We also reserve a region of physical memory as a buffer for future cross core communication and store the physical address and size of it in the \lstinline!global! struct.

In \lstinline!arm_kernel_startup! we do a static split of the available physical memory.
Each init thus only gets a capability to half of the memory, which prevents them from messing around with each others address space.
Finally, we also create the capability to the shared frame in this functions and store it in the \lstinline!TASKCN_SLOT_MON_URPC! slot.

\subsubsection{Cross core communication}



\todo {}

- Spawning cores on low level.
- Setup of ELF image
- Communication between cores

\subsection{FAT file system}
- filesystem code in aos\_support/fat32.c
- handler in mmchs/filesystem\_server.c
- Currently read-only
- some support for partition tables
- Performance improvements: sequential read? cached FAT? bulk transfer?

\subsubsection{ ELF loading}

\todo {Implement ELF loading}

\section{Conclusion}

\begin{flushleft}
{{{
\bibliographystyle {plain}
\bibliography {./references}
}}}
\end{flushleft}


\todos

\end{document}          
 
